{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import display, Audio\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1+cu121\n"
     ]
    }
   ],
   "source": [
    "# Print the version of pytorch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio File Analysis and Channel Conversion\n",
    "\n",
    "In this section, we analyze the audio files for Participant 1 to determine their characteristics and prepare them for further processing. The tasks include:\n",
    "\n",
    "1. **Loading Audio Files**: We load audio files listed in the `p01_df` dataframe to analyze their properties such as duration and number of channels.\n",
    "\n",
    "2. **Duration Calculation**: We compute the duration of each audio file in seconds and determine the maximum length among all recordings. This information is useful for understanding the variability in recording lengths.\n",
    "\n",
    "3. **Channel Analysis**: We check the number of channels for each audio file to distinguish between mono and stereo recordings. Mono recordings have one channel, while stereo recordings have two.\n",
    "\n",
    "4. **Conversion to Mono**: For consistency and ease of processing, we define a function `convert_to_mono()` to convert stereo audio files to mono by averaging the two channels. This step ensures that all audio files have a uniform format, which is beneficial for subsequent analysis and model training.\n",
    "\n",
    "The analysis of audio files and the conversion to mono format are essential preprocessing steps that help standardize the data for machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p01_df = pd.read_csv('data/p01_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an audio file and return the signal as a tensor and the sample rate\n",
    "signal, sample_rate = torchaudio.load('data/wav/' + p01_df['Filename'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1637, -0.1676, -0.1750,  ...,  0.0085,  0.0076,  0.0070],\n",
       "        [-0.1284, -0.1376, -0.1444,  ...,  0.0056,  0.0053,  0.0050]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length of the audio files is:  14.057006802721089 seconds\n"
     ]
    }
   ],
   "source": [
    "durations = []\n",
    "\n",
    "for i in range(len(p01_df)):\n",
    "    signal, sample_rate = torchaudio.load('data/wav/' + p01_df['Filename'].iloc[i])\n",
    "    durations.append(signal.size(1) / sample_rate)\n",
    "    \n",
    "# Find the maximum length in milliseconds of the audio files\n",
    "max_length = max(durations)\n",
    "\n",
    "print(\"The maximum length of the audio files is: \", max_length, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mono audio files: 314\n",
      "Number of stereo audio files: 1394\n"
     ]
    }
   ],
   "source": [
    "# Assuming p01_df['Filename'] contains the filenames of the audio files\n",
    "audio_dir = 'data/wav/'  # Replace with the path to your audio directory\n",
    "\n",
    "# Initialize counters for mono and stereo files\n",
    "mono_count = 0\n",
    "stereo_count = 0\n",
    "\n",
    "# Iterate over each file in the DataFrame\n",
    "for filename in p01_df['Filename']:\n",
    "    filepath = os.path.join(audio_dir, filename)\n",
    "\n",
    "    # Load the audio file\n",
    "    signal, sample_rate = torchaudio.load(filepath)\n",
    "\n",
    "    # Check the number of channels\n",
    "    if signal.shape[0] == 1:\n",
    "        mono_count += 1\n",
    "    elif signal.shape[0] == 2:\n",
    "        stereo_count += 1\n",
    "\n",
    "# Output the results\n",
    "print(f\"Number of mono audio files: {mono_count}\")\n",
    "print(f\"Number of stereo audio files: {stereo_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_mono(signal):\n",
    "    if signal.shape[0] == 2:  # If the signal has 2 channels (stereo)\n",
    "        signal = signal.mean(dim=0, keepdim=True)  # Convert to mono by averaging the channels\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Participant</th>\n",
       "      <th>Label</th>\n",
       "      <th>Audio</th>\n",
       "      <th>delighted</th>\n",
       "      <th>dysregulated</th>\n",
       "      <th>frustrated</th>\n",
       "      <th>request</th>\n",
       "      <th>selftalk</th>\n",
       "      <th>social</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200126_2142_00-13-04.06--00-13-04.324.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>dysregulated</td>\n",
       "      <td>[-0.14604187 -0.15263367 -0.15974426 ...  0.00...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200126_2142_00-06-41.54--00-06-42.47.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>dysregulated</td>\n",
       "      <td>[ 0.08834839  0.09138489  0.09321594 ... -0.12...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200126_2142_00-11-35.94--00-11-37.08.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>dysregulated</td>\n",
       "      <td>[0.0358429  0.02403259 0.01158142 ... 0.245162...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200126_2142_00-12-11.66--00-12-15.31.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>dysregulated</td>\n",
       "      <td>[ 0.00675964 -0.00045776 -0.01092529 ...  0.09...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200126_2142_00-00-24.55--00-00-24.95.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>dysregulated</td>\n",
       "      <td>[ 0.02839661  0.02764893  0.0249939  ... -0.29...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Filename Participant         Label  \\\n",
       "0  200126_2142_00-13-04.06--00-13-04.324.wav         P01  dysregulated   \n",
       "1   200126_2142_00-06-41.54--00-06-42.47.wav         P01  dysregulated   \n",
       "2   200126_2142_00-11-35.94--00-11-37.08.wav         P01  dysregulated   \n",
       "3   200126_2142_00-12-11.66--00-12-15.31.wav         P01  dysregulated   \n",
       "4   200126_2142_00-00-24.55--00-00-24.95.wav         P01  dysregulated   \n",
       "\n",
       "                                               Audio  delighted  dysregulated  \\\n",
       "0  [-0.14604187 -0.15263367 -0.15974426 ...  0.00...      False          True   \n",
       "1  [ 0.08834839  0.09138489  0.09321594 ... -0.12...      False          True   \n",
       "2  [0.0358429  0.02403259 0.01158142 ... 0.245162...      False          True   \n",
       "3  [ 0.00675964 -0.00045776 -0.01092529 ...  0.09...      False          True   \n",
       "4  [ 0.02839661  0.02764893  0.0249939  ... -0.29...      False          True   \n",
       "\n",
       "   frustrated  request  selftalk  social  \n",
       "0       False    False     False   False  \n",
       "1       False    False     False   False  \n",
       "2       False    False     False   False  \n",
       "3       False    False     False   False  \n",
       "4       False    False     False   False  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p01_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Data Augmentation and Dataset Preparation\n",
    "\n",
    "In this section, we continue with the data augmentation process and prepare the dataset for model training. The steps include:\n",
    "\n",
    "1. **Audio Data Augmentation**: We augment the audio files by applying padding, time-shifting, and generating mel spectrograms. This augmentation introduces variability and helps improve the robustness of machine learning models.\n",
    "\n",
    "2. **Adding Augmented Data to DataFrame**: The augmented audio signals and mel spectrograms are added to the `p01_df` dataframe to facilitate further analysis and model training.\n",
    "\n",
    "3. **Train-Test Split**: We create an 80:20 train-test split of the dataset using the augmented audio files and mel spectrograms. The split is stratified based on the original labels to ensure balanced representation of classes in both the training and testing datasets.\n",
    "\n",
    "4. **Dataset Saving**: The prepared dataframe is saved to a CSV file for future use, preserving the augmented data and corresponding labels for model development.\n",
    "\n",
    "This section finalizes the data preparation phase by ensuring the augmented data is correctly formatted and split into training and testing sets, ready for machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum length in samples\n",
    "max_length_samples = int(max_length * sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead, define a function to pad the signal to the maximum length\n",
    "\n",
    "def pad_signal(signal, max_length_samples):\n",
    "    if signal.shape[1] < max_length_samples:\n",
    "        pad_begin_len = random.randint(0, max_length_samples - signal.shape[1]) # Begin padding length can be anything between 0 and the difference between the max length and the signal length\n",
    "        pad_end_len = max_length_samples - signal.shape[1] - pad_begin_len  # End padding length is the difference between the max length and the sum of the signal length and the begin padding length\n",
    "\n",
    "        # Pad with zeros\n",
    "        pad_begin = torch.zeros(signal.shape[0], pad_begin_len)\n",
    "        pad_end = torch.zeros(signal.shape[0], pad_end_len)\n",
    "\n",
    "        signal = torch.cat((pad_begin, signal, pad_end), 1)\n",
    "        \n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to shift the signal in time by a random amount between -0.5 and 0.5 seconds\n",
    "\n",
    "def shift_signal(signal, sample_rate):\n",
    "    shift_amount = random.uniform(-0.5, 0.5)\n",
    "    shift_samples = int(shift_amount * sample_rate) # Convert the shift amount to samples\n",
    "    \n",
    "    if shift_samples > 0:\n",
    "        # Shift the signal to the right\n",
    "        signal = torch.cat((torch.zeros(signal.shape[0], shift_samples), signal[:, :-shift_samples]), 1)\n",
    "    else:\n",
    "        # Shift the signal to the left\n",
    "        signal = torch.cat((signal[:, -shift_samples:], torch.zeros(signal.shape[0], -shift_samples)), 1)\n",
    "        \n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate mel spectrograms from the audio files\n",
    "\n",
    "def generate_mel_spectrogram(signal, sample_rate, n_mels=128, fmin=0, fmax=None, n_fft=2048):\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_mels=n_mels, f_min=fmin, f_max=fmax, n_fft=n_fft)(signal)\n",
    "    mel_spectrogram_db = torchaudio.transforms.AmplitudeToDB()(mel_spectrogram)\n",
    "    \n",
    "    return mel_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to do time and frequency masking on the mel spectrogram\n",
    "\n",
    "def mask_mel_spectrogram(mel_spectrogram, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "    n_mel_channels = mel_spectrogram.shape[1] # Number of mel channels\n",
    "    n_mel_frames = mel_spectrogram.shape[2] # Number of mel frames\n",
    "    \n",
    "    max_mask_size_freq = int(max_mask_pct * n_mel_channels) # Maximum size of the frequency mask\n",
    "    max_mask_size_time = int(max_mask_pct * n_mel_frames) # Maximum size of the time mask\n",
    "    \n",
    "    for _ in range(n_freq_masks):\n",
    "        mask_size_freq = random.randint(0, max_mask_size_freq) # Random size of the frequency mask\n",
    "        mask_start_freq = random.randint(0, n_mel_channels - mask_size_freq) # Random start of the frequency mask\n",
    "        mel_spectrogram[:, mask_start_freq:mask_start_freq + mask_size_freq, :] = 0 \n",
    "        \n",
    "    for _ in range(n_time_masks):\n",
    "        mask_size_time = random.randint(0, max_mask_size_time) # Random size of the time mask\n",
    "        mask_start_time = random.randint(0, n_mel_frames - mask_size_time) # Random start of the time mask\n",
    "        mel_spectrogram[:, :, mask_start_time:mask_start_time + mask_size_time] = 0\n",
    "        \n",
    "    return mel_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to augment the audio files\n",
    "\n",
    "def augment_audio_files(audio_files, max_length_samples, sample_rate, n_mels=128, fmin=0, fmax=None, n_fft=2048, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "    mel_spectrograms = []\n",
    "    augmented_audio_files = [] # To store the augmented audio files\n",
    "    \n",
    "    for i in range(len(audio_files)):\n",
    "        signal = torch.tensor(audio_files[i])\n",
    "        signal = convert_to_mono(signal)\n",
    "        \n",
    "        # Check if conversion to mono was successful\n",
    "        if signal.shape[0] != 1:\n",
    "            print(f\"Warning: Signal {i} is not mono after conversion. Shape: {signal.shape}\")\n",
    "\n",
    "        signal = pad_signal(signal, max_length_samples)\n",
    "        signal = shift_signal(signal, sample_rate)\n",
    "        \n",
    "        augmented_audio_files.append(signal.numpy()) # Append the augmented audio file\n",
    "        \n",
    "        mel_spectrogram = generate_mel_spectrogram(signal, sample_rate, n_mels, fmin, fmax, n_fft)\n",
    "        mel_spectrogram = mask_mel_spectrogram(mel_spectrogram, max_mask_pct, n_freq_masks, n_time_masks)\n",
    "        mel_spectrograms.append(mel_spectrogram)\n",
    "        \n",
    "    return mel_spectrograms, augmented_audio_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1310657/2902082806.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  signal = torch.tensor(audio_files[i])\n"
     ]
    }
   ],
   "source": [
    "# Augment the audio files\n",
    "\n",
    "audio_files = [] # List to store the audio files because apprently, csv files don't store the audio files as tensors\n",
    "for i in range(len(p01_df)):\n",
    "    signal, sample_rate = torchaudio.load('data/wav/' + p01_df['Filename'].iloc[i])\n",
    "    audio_files.append(signal)\n",
    "sample_rate = librosa.load('data/wav/' + p01_df['Filename'].iloc[0], sr=None)[1]\n",
    "mel_spectrograms, augmented_signals = augment_audio_files(audio_files, max_length_samples, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 80:20 train and test split for the dataset for participant 1 using the augmented audio files and mel spectrograms\n",
    "\n",
    "# Add the mel spectrograms and augmented audio files to the dataframe\n",
    "p01_df['Mel Spectrogram'] = mel_spectrograms\n",
    "p01_df['Augmented Audio'] = augmented_signals\n",
    "\n",
    "# Save the dataframe to a csv file\n",
    "# p01_df.to_csv('data/p01_df_augmented.csv', index=False) # Index is set to False to avoid saving the index column\n",
    "\n",
    "# Make the train and test split for the dataset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(p01_df[['Augmented Audio', 'Mel Spectrogram']], p01_df.drop(['Augmented Audio', 'Mel Spectrogram'], axis=1), test_size=0.2, random_state=42, shuffle=True, stratify=p01_df['Label'])\n",
    "# Stratify the split based on the labels and not the one-hot encoded labels as the one-hot encoded labels are not present in the dataframe anymore and drop the augmented audio and mel spectrogram columns from the x dataframes as they are not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1366, 2), (342, 2), (1366, 10), (342, 10))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Augmented Audio</th>\n",
       "      <th>Mel Spectrogram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1693</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[[tensor(0.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>342 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Augmented Audio  \\\n",
       "1186  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1626  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1059  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "866   [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "990   [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "...                                                 ...   \n",
       "1677  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1707  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1194  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1693  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "608   [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "\n",
       "                                        Mel Spectrogram  \n",
       "1186  [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "1626  [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "1059  [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "866   [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "990   [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "...                                                 ...  \n",
       "1677  [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "1707  [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "1194  [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "1693  [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "608   [[[tensor(0.), tensor(0.), tensor(0.), tensor(...  \n",
       "\n",
       "[342 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Participant</th>\n",
       "      <th>Label</th>\n",
       "      <th>Audio</th>\n",
       "      <th>delighted</th>\n",
       "      <th>dysregulated</th>\n",
       "      <th>frustrated</th>\n",
       "      <th>request</th>\n",
       "      <th>selftalk</th>\n",
       "      <th>social</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>200329_1113_00-13-38.86--00-13-39.94.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>selftalk</td>\n",
       "      <td>[ 0.0186615   0.00944519  0.0038147  ... -0.01...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>200309_2035_00-05-10.63--00-05-11.91.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>delighted</td>\n",
       "      <td>[-0.00149536 -0.00054932 -0.00024414 ... -0.00...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>200229_2244_00-05-22.03--00-05-23.17.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>selftalk</td>\n",
       "      <td>[-0.03457642 -0.03807068 -0.04052734 ...  0.00...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>200306_2024_00-03-20.26--00-03-20.91.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>selftalk</td>\n",
       "      <td>[ 0.01483154  0.01954651  0.02474976 ...  0.00...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>200306_2024_00-17-31.11--00-17-31.68.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>selftalk</td>\n",
       "      <td>[ 0.11328125  0.10844421  0.1025238  ... -0.01...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>200815_2140_00-00-15.88--00-00-18.55.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>delighted</td>\n",
       "      <td>[ 0.00408936  0.0065918   0.01071167 ... -0.03...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>200307_1826_00-05-53.14--00-05-53.71.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>delighted</td>\n",
       "      <td>[ 0.00387573  0.0035553   0.00352478 ... -0.00...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>200306_2024_00-08-21.67--00-08-24.98.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>selftalk</td>\n",
       "      <td>[-0.01591492 -0.01504517 -0.01268005 ... -0.00...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1693</th>\n",
       "      <td>200529_1029_00-01-10.01--00-01-11.35.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>delighted</td>\n",
       "      <td>[-6.1035156e-04 -3.0517578e-05  9.1552734e-04 ...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>210324_2036_00-06-12.32--00-06-13.43.wav</td>\n",
       "      <td>P01</td>\n",
       "      <td>social</td>\n",
       "      <td>[-0.00427246 -0.00418091 -0.00411987 ...  0.00...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>342 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Filename Participant      Label  \\\n",
       "1186  200329_1113_00-13-38.86--00-13-39.94.wav         P01   selftalk   \n",
       "1626  200309_2035_00-05-10.63--00-05-11.91.wav         P01  delighted   \n",
       "1059  200229_2244_00-05-22.03--00-05-23.17.wav         P01   selftalk   \n",
       "866   200306_2024_00-03-20.26--00-03-20.91.wav         P01   selftalk   \n",
       "990   200306_2024_00-17-31.11--00-17-31.68.wav         P01   selftalk   \n",
       "...                                        ...         ...        ...   \n",
       "1677  200815_2140_00-00-15.88--00-00-18.55.wav         P01  delighted   \n",
       "1707  200307_1826_00-05-53.14--00-05-53.71.wav         P01  delighted   \n",
       "1194  200306_2024_00-08-21.67--00-08-24.98.wav         P01   selftalk   \n",
       "1693  200529_1029_00-01-10.01--00-01-11.35.wav         P01  delighted   \n",
       "608   210324_2036_00-06-12.32--00-06-13.43.wav         P01     social   \n",
       "\n",
       "                                                  Audio  delighted  \\\n",
       "1186  [ 0.0186615   0.00944519  0.0038147  ... -0.01...      False   \n",
       "1626  [-0.00149536 -0.00054932 -0.00024414 ... -0.00...       True   \n",
       "1059  [-0.03457642 -0.03807068 -0.04052734 ...  0.00...      False   \n",
       "866   [ 0.01483154  0.01954651  0.02474976 ...  0.00...      False   \n",
       "990   [ 0.11328125  0.10844421  0.1025238  ... -0.01...      False   \n",
       "...                                                 ...        ...   \n",
       "1677  [ 0.00408936  0.0065918   0.01071167 ... -0.03...       True   \n",
       "1707  [ 0.00387573  0.0035553   0.00352478 ... -0.00...       True   \n",
       "1194  [-0.01591492 -0.01504517 -0.01268005 ... -0.00...      False   \n",
       "1693  [-6.1035156e-04 -3.0517578e-05  9.1552734e-04 ...       True   \n",
       "608   [-0.00427246 -0.00418091 -0.00411987 ...  0.00...      False   \n",
       "\n",
       "      dysregulated  frustrated  request  selftalk  social  \n",
       "1186         False       False    False      True   False  \n",
       "1626         False       False    False     False   False  \n",
       "1059         False       False    False      True   False  \n",
       "866          False       False    False      True   False  \n",
       "990          False       False    False      True   False  \n",
       "...            ...         ...      ...       ...     ...  \n",
       "1677         False       False    False     False   False  \n",
       "1707         False       False    False     False   False  \n",
       "1194         False       False    False      True   False  \n",
       "1693         False       False    False     False   False  \n",
       "608          False       False    False     False    True  \n",
       "\n",
       "[342 rows x 10 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename        object\n",
      "Participant     object\n",
      "Label           object\n",
      "Audio           object\n",
      "delighted         bool\n",
      "dysregulated      bool\n",
      "frustrated        bool\n",
      "request           bool\n",
      "selftalk          bool\n",
      "social            bool\n",
      "dtype: object\n",
      "                                        Filename Participant         Label  \\\n",
      "348     200124_1828_00-09-14.62--00-09-15.67.wav         P01  dysregulated   \n",
      "85    200229_2156_00-08-01.665--00-08-03.284.wav         P01    frustrated   \n",
      "1526    200309_2035_00-01-53.76--00-01-54.45.wav         P01     delighted   \n",
      "393    200124_1828_00-01-01.464--00-01-02.72.wav         P01  dysregulated   \n",
      "1080     200306_2024_00-02-28.21--00-02-29.9.wav         P01      selftalk   \n",
      "\n",
      "                                                  Audio  delighted  \\\n",
      "348   [-0.05300903 -0.06085205 -0.06741333 ... -0.22...      False   \n",
      "85    [-0.0071106  -0.00773621 -0.00802612 ... -0.00...      False   \n",
      "1526  [-5.6457520e-03 -5.6152344e-03 -5.5236816e-03 ...       True   \n",
      "393   [ 0.13546753  0.18415833  0.2346344  ... -0.11...      False   \n",
      "1080  [-0.00106812 -0.00361633 -0.00590515 ... -0.00...      False   \n",
      "\n",
      "      dysregulated  frustrated  request  selftalk  social  \n",
      "348           True       False    False     False   False  \n",
      "85           False        True    False     False   False  \n",
      "1526         False       False    False     False   False  \n",
      "393           True       False    False     False   False  \n",
      "1080         False       False    False      True   False  \n"
     ]
    }
   ],
   "source": [
    "print(y_train.dtypes)  # Check the data types of the columns in y_train\n",
    "print(y_train.head())  # Display the first few rows of y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data for PyTorch Model Training\n",
    "\n",
    "In this section, we prepare the dataset for training a PyTorch model by performing the following steps:\n",
    "\n",
    "1. **Label Indexing**: We define a function `get_label_index()` to convert multi-label classifications into a single label index for each sample. This index represents the class of each audio file, making it suitable for training classification models in PyTorch.\n",
    "\n",
    "2. **Converting Data to PyTorch Tensors**: The mel spectrograms and labels for both training and testing datasets are converted into PyTorch tensors. This conversion is necessary for efficient batch processing and compatibility with PyTorch models.\n",
    "\n",
    "3. **Creating DataLoaders**: We define a function `create_dataloaders()` that creates PyTorch `DataLoader` objects for the training and validation datasets. These `DataLoader` objects facilitate easy batch processing and shuffling of data during training, improving the efficiency and performance of the model training process.\n",
    "\n",
    "This setup ensures that the data is in the correct format and ready for training machine learning models using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the index of the label\n",
    "def get_label_index(row):\n",
    "    labels = ['delighted', 'dysregulated', 'frustrated', 'request', 'selftalk', 'social']\n",
    "    for idx, label in enumerate(labels):\n",
    "        if row[label]:  # If the label is True, return its index\n",
    "            return idx\n",
    "    return -1  # If no label is True, return an invalid index (this shouldn't happen in a clean dataset)\n",
    "\n",
    "# Apply the function to create a label index for each row\n",
    "y_train['Label_Index'] = y_train.apply(get_label_index, axis=1)\n",
    "y_test['Label_Index'] = y_test.apply(get_label_index, axis=1)\n",
    "\n",
    "# Convert to tensors for PyTorch\n",
    "train_labels = torch.tensor(y_train['Label_Index'].values).long()\n",
    "test_labels = torch.tensor(y_test['Label_Index'].values).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building, Training, and Evaluating the Audio Classification Model\n",
    "\n",
    "### Model Architecture: `AudioClassifier`\n",
    "\n",
    "The `AudioClassifier` is a deep convolutional neural network designed for classifying audio data, specifically mel spectrograms. It follows a standard architecture of convolutional layers followed by fully connected layers, tailored for audio classification tasks. Here is a breakdown of the architecture:\n",
    "\n",
    "1. **Convolutional Layers**: The model contains four convolutional layers (`conv1` to `conv4`) with increasing filter sizes. These layers are used to automatically learn spatial hierarchies of features from the input mel spectrograms:\n",
    "   - **`conv1`**: Takes input with 1 channel (mel spectrograms) and outputs 32 feature maps. Uses a kernel size of 3x3 with a stride of 1 and padding of 1 to maintain the input size.\n",
    "   - **`conv2`**: Takes 32 input channels and outputs 64 feature maps, maintaining the same kernel size, stride, and padding.\n",
    "   - **`conv3`**: Takes 64 input channels and outputs 128 feature maps.\n",
    "   - **`conv4`**: Takes 128 input channels and outputs 256 feature maps.\n",
    "   \n",
    "2. **Pooling Layers**: After each convolutional layer, a Max Pooling layer (`pool`) with a kernel size of 2x2 and stride 2 is applied. This reduces the dimensionality of the feature maps by a factor of 2, capturing the most important features while reducing computational cost.\n",
    "\n",
    "3. **Global Average Pooling Layer**: After the convolutional layers, a global average pooling layer (`global_avg_pool`) is applied. This layer reduces each feature map to a single number by taking the average of all the values, reducing the model’s sensitivity to spatial translations of features in the input.\n",
    "\n",
    "4. **Fully Connected Layers**: Following the convolutional and pooling layers, there are two fully connected layers:\n",
    "   - **`fc1`**: A linear layer with 256 inputs (the output from the global average pooling) and 512 outputs. This layer introduces non-linearity to the model using ReLU activation and prepares features for classification.\n",
    "   - **`fc2`**: The final output layer that maps the 512-dimensional input to `n_classes` output nodes, where `n_classes` is the number of unique labels in the dataset. This layer outputs logits that represent the raw, unnormalized scores for each class.\n",
    "\n",
    "5. **Dropout Layer**: A dropout layer (`dropout`) with a dropout probability of 0.5 is used before the final fully connected layer to prevent overfitting by randomly setting half of the input units to zero during training.\n",
    "\n",
    "### Data Preparation: Creating Dataloaders\n",
    "\n",
    "The function `create_dataloaders()` prepares the data for training by converting the mel spectrograms and labels into PyTorch tensors and wrapping them in `TensorDataset` objects. The function also adds a channel dimension to the spectrograms, making them compatible with the input requirements of convolutional layers. The resulting datasets are then loaded into `DataLoader` objects, which allow for efficient batch processing and shuffling of the data during training and evaluation.\n",
    "\n",
    "### Training the Model: `train_model`\n",
    "\n",
    "The function `train_model()` is responsible for training the `AudioClassifier` model. Key components of the training loop include:\n",
    "\n",
    "1. **Training Mode**: The model is set to training mode with `model.train()`, enabling dropout and batch normalization layers (if any).\n",
    "\n",
    "2. **Batch Processing**: For each batch of data, the model performs forward and backward passes:\n",
    "   - **Forward Pass**: The input batch is passed through the model to get predictions (`y_pred`).\n",
    "   - **Loss Calculation**: The difference between predictions and actual labels is calculated using the cross-entropy loss function (`criterion`).\n",
    "   - **Backward Pass**: The gradients of the loss with respect to model parameters are calculated using backpropagation (`loss.backward()`).\n",
    "   - **Optimizer Step**: The optimizer updates the model parameters based on the calculated gradients (`optimizer.step()`).\n",
    "\n",
    "3. **Validation**: After each epoch, the model is evaluated on the validation set to monitor performance. Metrics such as loss, accuracy, and F1 score are calculated.\n",
    "\n",
    "4. **Learning Rate Scheduler**: The learning rate is adjusted using `scheduler.step(val_loss)` if the validation loss does not improve for a specified number of epochs (patience).\n",
    "\n",
    "### Evaluating the Model: `test_model`\n",
    "\n",
    "The function `test_model()` evaluates the trained model on a test set to assess its generalization performance. The test accuracy and F1 score are calculated and printed to provide insights into the model's effectiveness on unseen data.\n",
    "\n",
    "### Hyperparameters and Setup\n",
    "\n",
    "- **`n_classes`**: Number of unique labels in the dataset.\n",
    "- **`n_epochs`**: Number of training epochs, set to 40.\n",
    "- **`batch_size`**: Number of samples per batch, set to 32.\n",
    "- **`lr`**: Learning rate for the optimizer, set to 0.001.\n",
    "- **`device`**: The device on which the model will run, either GPU (`cuda`) if available or CPU.\n",
    "\n",
    "### Model Initialization and Training\n",
    "\n",
    "- **Model Creation**: An instance of `AudioClassifier` is created with the specified number of classes.\n",
    "- **Loss Function**: Cross-entropy loss is used to compute the difference between predictions and ground truth labels.\n",
    "- **Optimizer**: Adam optimizer is chosen for its efficiency and adaptive learning rate capabilities.\n",
    "- **Learning Rate Scheduler**: `ReduceLROnPlateau` scheduler is used to reduce the learning rate when the validation loss plateaus.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This comprehensive setup, involving the definition of a deep learning model, data preparation, training, and evaluation processes, is aimed at building a robust audio classification system using PyTorch. The model is designed to classify audio signals into predefined categories based on mel spectrograms, leveraging convolutional layers to extract features and fully connected layers for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1310657/2972664145.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_mels = torch.stack([torch.tensor(ms, dtype=torch.float32) for ms in X_train['Mel Spectrogram']])\n",
      "/tmp/ipykernel_1310657/2972664145.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_mels = torch.stack([torch.tensor(ms, dtype=torch.float32) for ms in X_test['Mel Spectrogram']])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train Loss: 0.06651 | Train Acc: 0.26647 | Val Loss: 0.05352 | Val Acc: 0.30702 | F1 Score: 0.17909\n",
      "Epoch: 1 | Train Loss: 0.05149 | Train Acc: 0.33309 | Val Loss: 0.05173 | Val Acc: 0.33041 | F1 Score: 0.16412\n",
      "Epoch: 2 | Train Loss: 0.05101 | Train Acc: 0.33016 | Val Loss: 0.05133 | Val Acc: 0.33626 | F1 Score: 0.21876\n",
      "Epoch: 3 | Train Loss: 0.04962 | Train Acc: 0.35212 | Val Loss: 0.04998 | Val Acc: 0.36257 | F1 Score: 0.25781\n",
      "Epoch: 4 | Train Loss: 0.04780 | Train Acc: 0.39458 | Val Loss: 0.05030 | Val Acc: 0.36550 | F1 Score: 0.24149\n",
      "Epoch: 5 | Train Loss: 0.04703 | Train Acc: 0.39531 | Val Loss: 0.04669 | Val Acc: 0.38889 | F1 Score: 0.26798\n",
      "Epoch: 6 | Train Loss: 0.04410 | Train Acc: 0.42094 | Val Loss: 0.04556 | Val Acc: 0.42690 | F1 Score: 0.32394\n",
      "Epoch: 7 | Train Loss: 0.04324 | Train Acc: 0.43924 | Val Loss: 0.04688 | Val Acc: 0.35673 | F1 Score: 0.31946\n",
      "Epoch: 8 | Train Loss: 0.04133 | Train Acc: 0.46193 | Val Loss: 0.04376 | Val Acc: 0.40936 | F1 Score: 0.38439\n",
      "Epoch: 9 | Train Loss: 0.04219 | Train Acc: 0.44510 | Val Loss: 0.04427 | Val Acc: 0.45029 | F1 Score: 0.37570\n",
      "Epoch: 10 | Train Loss: 0.03780 | Train Acc: 0.50000 | Val Loss: 0.04057 | Val Acc: 0.45906 | F1 Score: 0.41982\n",
      "Epoch: 11 | Train Loss: 0.03668 | Train Acc: 0.51025 | Val Loss: 0.04185 | Val Acc: 0.46199 | F1 Score: 0.44291\n",
      "Epoch: 12 | Train Loss: 0.03386 | Train Acc: 0.55051 | Val Loss: 0.04522 | Val Acc: 0.46784 | F1 Score: 0.42395\n",
      "Epoch: 13 | Train Loss: 0.03415 | Train Acc: 0.55124 | Val Loss: 0.04134 | Val Acc: 0.50000 | F1 Score: 0.47927\n",
      "Epoch: 14 | Train Loss: 0.03153 | Train Acc: 0.61274 | Val Loss: 0.04329 | Val Acc: 0.51170 | F1 Score: 0.47463\n",
      "Epoch: 15 | Train Loss: 0.02757 | Train Acc: 0.64348 | Val Loss: 0.04515 | Val Acc: 0.48538 | F1 Score: 0.46592\n",
      "Epoch: 16 | Train Loss: 0.02717 | Train Acc: 0.66105 | Val Loss: 0.04644 | Val Acc: 0.49123 | F1 Score: 0.47816\n",
      "Epoch: 17 | Train Loss: 0.02233 | Train Acc: 0.74158 | Val Loss: 0.04449 | Val Acc: 0.52632 | F1 Score: 0.52043\n",
      "Epoch: 18 | Train Loss: 0.01952 | Train Acc: 0.78184 | Val Loss: 0.04586 | Val Acc: 0.53509 | F1 Score: 0.52901\n",
      "Epoch: 19 | Train Loss: 0.01828 | Train Acc: 0.79575 | Val Loss: 0.04773 | Val Acc: 0.52924 | F1 Score: 0.52662\n",
      "Epoch: 20 | Train Loss: 0.01707 | Train Acc: 0.80600 | Val Loss: 0.04920 | Val Acc: 0.52632 | F1 Score: 0.52511\n",
      "Epoch: 21 | Train Loss: 0.01607 | Train Acc: 0.81406 | Val Loss: 0.05007 | Val Acc: 0.52047 | F1 Score: 0.52155\n",
      "Epoch: 22 | Train Loss: 0.01515 | Train Acc: 0.83821 | Val Loss: 0.04974 | Val Acc: 0.53801 | F1 Score: 0.53475\n",
      "Epoch: 23 | Train Loss: 0.01461 | Train Acc: 0.83455 | Val Loss: 0.05014 | Val Acc: 0.52924 | F1 Score: 0.52887\n",
      "Epoch: 24 | Train Loss: 0.01437 | Train Acc: 0.84553 | Val Loss: 0.05076 | Val Acc: 0.52339 | F1 Score: 0.52276\n",
      "Epoch: 25 | Train Loss: 0.01436 | Train Acc: 0.84041 | Val Loss: 0.05092 | Val Acc: 0.52339 | F1 Score: 0.52293\n",
      "Epoch: 26 | Train Loss: 0.01427 | Train Acc: 0.84407 | Val Loss: 0.05119 | Val Acc: 0.52339 | F1 Score: 0.52228\n",
      "Epoch: 27 | Train Loss: 0.01393 | Train Acc: 0.84553 | Val Loss: 0.05129 | Val Acc: 0.52632 | F1 Score: 0.52535\n",
      "Epoch: 28 | Train Loss: 0.01423 | Train Acc: 0.84773 | Val Loss: 0.05143 | Val Acc: 0.52339 | F1 Score: 0.52293\n",
      "Epoch: 29 | Train Loss: 0.01371 | Train Acc: 0.85359 | Val Loss: 0.05147 | Val Acc: 0.52339 | F1 Score: 0.52293\n",
      "Epoch: 30 | Train Loss: 0.01416 | Train Acc: 0.84919 | Val Loss: 0.05150 | Val Acc: 0.52339 | F1 Score: 0.52293\n",
      "Epoch: 31 | Train Loss: 0.01371 | Train Acc: 0.85139 | Val Loss: 0.05152 | Val Acc: 0.52339 | F1 Score: 0.52293\n",
      "Epoch: 32 | Train Loss: 0.01388 | Train Acc: 0.84334 | Val Loss: 0.05153 | Val Acc: 0.52339 | F1 Score: 0.52293\n",
      "Epoch: 33 | Train Loss: 0.01365 | Train Acc: 0.85505 | Val Loss: 0.05155 | Val Acc: 0.52339 | F1 Score: 0.52293\n",
      "Epoch: 34 | Train Loss: 0.01386 | Train Acc: 0.85359 | Val Loss: 0.05159 | Val Acc: 0.52339 | F1 Score: 0.52293\n",
      "Epoch: 35 | Train Loss: 0.01387 | Train Acc: 0.84993 | Val Loss: 0.05159 | Val Acc: 0.52339 | F1 Score: 0.52293\n",
      "Epoch: 36 | Train Loss: 0.01385 | Train Acc: 0.85359 | Val Loss: 0.05159 | Val Acc: 0.52339 | F1 Score: 0.52293\n",
      "Epoch: 37 | Train Loss: 0.01404 | Train Acc: 0.84627 | Val Loss: 0.05159 | Val Acc: 0.52339 | F1 Score: 0.52293\n",
      "Epoch: 38 | Train Loss: 0.01397 | Train Acc: 0.85286 | Val Loss: 0.05159 | Val Acc: 0.52339 | F1 Score: 0.52293\n",
      "Epoch: 39 | Train Loss: 0.01353 | Train Acc: 0.85286 | Val Loss: 0.05160 | Val Acc: 0.52339 | F1 Score: 0.52293\n",
      "Test Accuracy: 0.52339 | F1 Score: 0.52293\n"
     ]
    }
   ],
   "source": [
    "# Define the AudioClassifier model\n",
    "class AudioClassifier(torch.nn.Module):\n",
    "    def __init__(self, n_classes, n_mels=128):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv3 = torch.nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv4 = torch.nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
    "        self.global_avg_pool = torch.nn.AdaptiveAvgPool2d((1, 1))  # Global average pooling\n",
    "        self.fc1 = torch.nn.Linear(256, 512)  # Adjusted to the reduced size after GAP\n",
    "        self.fc2 = torch.nn.Linear(512, n_classes)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Check input dimensions and reshape if necessary\n",
    "        if x.dim() == 5:  # If input is 5D, reduce to 4D\n",
    "            x = x.squeeze(2)  # Remove the unnecessary dimension (batch size, channels, height, width)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.global_avg_pool(x)  # Apply Global Average Pooling\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def create_dataloaders(X_train, X_test, y_train, y_test, batch_size=32):\n",
    "    # Convert the mel-spectrograms to PyTorch tensors and add the channel dimension\n",
    "    train_mels = torch.stack([torch.tensor(ms, dtype=torch.float32) for ms in X_train['Mel Spectrogram']])\n",
    "    test_mels = torch.stack([torch.tensor(ms, dtype=torch.float32) for ms in X_test['Mel Spectrogram']])\n",
    "\n",
    "    # Ensure the spectrograms have the correct dimensions [batch_size, channels, height, width]\n",
    "    train_mels = train_mels.unsqueeze(1)  # Adding channel dimension: [batch_size, 1, height, width]\n",
    "    test_mels = test_mels.unsqueeze(1)    # Adding channel dimension: [batch_size, 1, height, width]\n",
    "\n",
    "    # Convert labels to numeric format\n",
    "    if isinstance(y_train, pd.DataFrame):\n",
    "        train_labels = torch.tensor(y_train['Label_Index'].values, dtype=torch.long)\n",
    "        test_labels = torch.tensor(y_test['Label_Index'].values, dtype=torch.long)\n",
    "    else:\n",
    "        train_labels = torch.tensor(y_train, dtype=torch.long)\n",
    "        test_labels = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    # Create TensorDataset\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_mels, train_labels)\n",
    "    val_dataset = torch.utils.data.TensorDataset(test_mels, test_labels)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, criterion, optimizer, scheduler, n_epochs, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(device), y.to(device)  # Move the data to the device\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            y_pred = model(X)  # Get the model's predictions\n",
    "            loss = criterion(y_pred, y)  # Calculate the loss\n",
    "            loss.backward()  # Backpropagate the loss\n",
    "            optimizer.step()  # Update the weights\n",
    "\n",
    "            train_loss += loss.item()  # Accumulate the loss\n",
    "            train_acc += (y_pred.argmax(1) == y).sum().item()  # Accumulate correct predictions\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)  # Average loss\n",
    "        train_acc /= len(train_loader.dataset)  # Average accuracy\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        y_true = []\n",
    "        y_pred_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                y_pred = model(X)\n",
    "                loss = criterion(y_pred, y)\n",
    "                val_loss += loss.item()\n",
    "                val_acc += (y_pred.argmax(1) == y).sum().item()\n",
    "                y_true.extend(y.cpu().numpy())\n",
    "                y_pred_list.extend(y_pred.argmax(1).cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc /= len(val_loader.dataset)\n",
    "\n",
    "        # Calculate F1 Score\n",
    "        f1 = f1_score(y_true, y_pred_list, average='weighted')\n",
    "        print(f\"Epoch: {epoch} | Train Loss: {train_loss:.5f} | Train Acc: {train_acc:.5f} | Val Loss: {val_loss:.5f} | Val Acc: {val_acc:.5f} | F1 Score: {f1:.5f}\")\n",
    "\n",
    "        scheduler.step(val_loss)  # Adjust learning rate\n",
    "\n",
    "# Function to test the model\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_acc = 0.0\n",
    "    y_true = []\n",
    "    y_pred_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model(X)\n",
    "            test_acc += (y_pred.argmax(1) == y).sum().item()\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "            y_pred_list.extend(y_pred.argmax(1).cpu().numpy())\n",
    "\n",
    "    test_acc /= len(test_loader.dataset)\n",
    "\n",
    "    # Calculate F1 Score\n",
    "    f1 = f1_score(y_true, y_pred_list, average='weighted')\n",
    "    print(f\"Test Accuracy: {test_acc:.5f} | F1 Score: {f1:.5f}\")\n",
    "\n",
    "# Define the hyperparameters\n",
    "n_classes = len(p01_df['Label'].unique())\n",
    "n_epochs = 40\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create the dataloaders\n",
    "train_loader, val_loader = create_dataloaders(X_train, X_test, y_train, y_test, batch_size)\n",
    "\n",
    "# Create the model, criterion, optimizer, and scheduler\n",
    "model = AudioClassifier(n_classes)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, criterion, optimizer, scheduler, n_epochs, train_loader, val_loader, device)\n",
    "\n",
    "# Test the model\n",
    "test_model(model, val_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
