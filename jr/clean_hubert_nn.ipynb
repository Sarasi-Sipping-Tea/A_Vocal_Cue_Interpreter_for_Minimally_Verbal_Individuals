{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "\n",
    "from helpers import get_hubert_features, unweighted_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ad029ec4d334136a38d9a5beaf2056d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/981 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get HuBERT features for one participant\n",
    "hubert_features = get_hubert_features(\"P05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = hubert_features[\"label_list\"]\n",
    "X_tr = hubert_features[\"X_tr\"]\n",
    "y_tr = hubert_features[\"y_tr\"]\n",
    "X_te = hubert_features[\"X_te\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jr/miniconda3/envs/ml_311/lib/python3.11/site-packages/torch/nn/modules/module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "/home/jr/miniconda3/envs/ml_311/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 44\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     40\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model(X_te)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mepoch_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magreement \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(pred\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39mpred_logistic\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax difference \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mabs(pred\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mpred_logistic)\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Test that logistic regression gives the same results\n",
    "# as a regularized neural network with no hidden layers\n",
    "C = 0.02\n",
    "est = make_pipeline(StandardScaler(), LogisticRegression(C=C, max_iter=10**8))\n",
    "est.fit(X_tr, y_tr)\n",
    "pred_logistic = est.predict_proba(X_te)\n",
    "\n",
    "model = nn.Sequential(nn.Linear(768, len(label_list)), nn.Softmax())\n",
    "opt = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "\n",
    "# Train the regularized network for many epochs. Every\n",
    "# 1000 epochs, we compare predictions on the test set\n",
    "# to the prediction from logistic regression, and we log\n",
    "# the fraction of samples for which the predictions agree,\n",
    "# and the maximum difference in predicted probabilities.\n",
    "# We expect the fraction to approach 1.0 and the maximum\n",
    "# difference to approach 0.0 as the number of training\n",
    "# epochs grows.\n",
    "num_epochs = 10000\n",
    "for epoch_number in range(num_epochs):\n",
    "    pred = model(X_tr)\n",
    "    std_by_feature = X_tr.std(0)\n",
    "\n",
    "    # Training loss is categorical cross-entropy plus an L^2\n",
    "    # regularization term\n",
    "    loss = nn.NLLLoss()(pred.log(), y_tr)\n",
    "    loss += (\n",
    "        (next(next(model.children()).parameters()) * std_by_feature) ** 2\n",
    "    ).sum() / (2 * C * len(X_tr))\n",
    "\n",
    "    # Update the weights\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    # Compare the networks predictions on test set to\n",
    "    # the logistic regression model\n",
    "    if (1 + epoch_number) % 1000 == 0:\n",
    "        with torch.no_grad():\n",
    "            pred = model(X_te)\n",
    "            print(\n",
    "                f\"Epoch {1+epoch_number}, \"\n",
    "                f\"agreement {(pred.numpy().argmax(1) == pred_logistic.argmax(1)).mean():,.4f}, \"\n",
    "                f\"max difference {np.abs(pred.numpy() - pred_logistic).max():,.4f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer networks\n",
    "\n",
    "Above, we saw that a network with no hidden layers and L^2 regularization is\n",
    "equivalent to regularized logistic regression. Next, we will add some hidden\n",
    "layers to the network. Since our dataset is relatively small, using a simple\n",
    "validation set to determine when to halt training will introduce large variance.\n",
    "Instead, we cross-validate using the following methodology:\n",
    " - Initialize network weights randomly\n",
    " - Clone the network for each CV fold\n",
    " - Train each network on its respective training folds, and make\n",
    "predictions on the respective test folds\n",
    " - Assemble out-of-sample test fold predictions and compute unweighted f1 score associated with each epoch\n",
    " - Halt the training once the out-of-sample unweighted f1 score peaks (we average scores over a rolling window of 9 consecutive epochs to reduce noise)\n",
    " - For inference on the test set, retrain a network starting with the same initial weights using the entire training set, using the determined number of training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "test_folds = np.random.randint(0, 10, len(X_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 16\n",
    "torch.manual_seed(12345)\n",
    "base_model = nn.Sequential(\n",
    "    nn.Linear(768, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, len(label_list)),\n",
    "    nn.Softmax(),\n",
    ")\n",
    "\n",
    "model_dict = {}\n",
    "opt_dict = {}\n",
    "for test_fold in np.unique(test_folds):\n",
    "    model = deepcopy(base_model)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=1e-2)\n",
    "    model_dict[test_fold] = model\n",
    "    opt_dict[test_fold] = opt\n",
    "\n",
    "scores = []\n",
    "for n in tqdm(range(300)):\n",
    "    pred = torch.zeros(len(X_tr), len(label_list))\n",
    "    for fold, model in model_dict.items():\n",
    "        opt = opt_dict[fold]\n",
    "        mask = test_folds == fold\n",
    "        std_by_feature = X_tr[~mask].std(0)\n",
    "        loss = nn.NLLLoss()(model(X_tr[~mask]).log(), y_tr[~mask])\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        with torch.no_grad():\n",
    "            pred[mask] = model(X_tr[mask])\n",
    "    score = multiclass_f1_score(\n",
    "        pred, y_tr, average=\"macro\", num_classes=len(label_list)\n",
    "    )\n",
    "    scores.append(score)\n",
    "scores = np.array(scores)\n",
    "smoothed_scores = sum(scores[n : -9 + n] for n in range(9)) / 9\n",
    "print(np.max(smoothed_scores), np.argmax(smoothed_scores) + 4)\n",
    "plt.plot(smoothed_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, len(label_list)),\n",
    "    nn.Softmax(),\n",
    ")\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-2)\n",
    "for n in tqdm(range(130)):\n",
    "    mask = test_folds == fold\n",
    "    std_by_feature = X_tr[~mask].std(0)\n",
    "    loss = nn.NLLLoss()(model(X_tr[~mask]).log(), y_tr[~mask])\n",
    "    loss += (\n",
    "        (next(next(model.children()).parameters()) * std_by_feature) ** 2\n",
    "    ).sum() / (2 * C * (~mask).sum())\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te, y_te = hubert_features[\"X_te\"], hubert_features[\"y_te\"]\n",
    "with torch.no_grad():\n",
    "    pred = model(X_te).argmax(1)\n",
    "print(\"f1 score, all test:\", unweighted_f1(y_te, pred))\n",
    "session_tr = hubert_features[\"session_tr\"]\n",
    "session_te = hubert_features[\"session_te\"]\n",
    "small_session_mask = ~pd.Series(session_te).isin(session_tr)\n",
    "print(\n",
    "    \"f1 score, holdout sessions:\",\n",
    "    unweighted_f1(y_te[small_session_mask], pred[small_session_mask]),\n",
    ")\n",
    "print(\n",
    "    \"f1 score, sessions shared with training data:\",\n",
    "    unweighted_f1(y_te[~small_session_mask], pred[~small_session_mask]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for test predictions\n",
    "conf_mat = confusion_matrix(pred, y_te)\n",
    "display(pd.DataFrame(conf_mat, label_list, label_list))\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(conf_mat, cmap=\"viridis\", interpolation=\"nearest\")\n",
    "plt.xticks(range(len(label_list)), labels=label_list)\n",
    "plt.yticks(range(len(label_list)), labels=label_list)\n",
    "plt.ylabel(\"Predicted label\")\n",
    "plt.xlabel(\"Actual label\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
